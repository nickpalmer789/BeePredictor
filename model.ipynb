{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that we need to do is import the csv file that contains all of our cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the CSV file with all of our data\n",
    "clean_data = pd.read_csv('data/cleaned/schwartau/schwartau.csv', encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to 'normalize' our timestamps. In order to do this, we are going to take its distance (1 step = 15 minute increment) from January 1st at time 00:00:00 in year 2017. This will give us an easier scale to base time off of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alter timestamp data to be normalized\n",
    "timestamps = np.array([x/clean_data.shape[0] for x in range(clean_data['timestamp'].shape[0])])\n",
    "#Get circular time data for extra feature\n",
    "circular_timestamps = np.array([])\n",
    "for stamp in clean_data['timestamp']:\n",
    "    stamp = np.datetime64(stamp)\n",
    "    seconds = stamp - np.datetime64('{}-01-01T00:00:00'.format(str(stamp.astype('datetime64[Y]').astype(int) + 1970)))\n",
    "    minutes = seconds.item().total_seconds() / 60\n",
    "    tick = minutes/15\n",
    "    circular_timestamps = np.append(circular_timestamps, tick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are going to create a numpy array to store our features in. Once our array is created, we are going to rotate the array by -90 degrees. This will result in rows with one entry from each column. Therefore, we are getting a list of features for every given timestamp. Then, we use Min Max Scalar to normalize our data between 0 and 1 without altering the distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a numpy array to store our features in\n",
    "X = np.array([\n",
    "    clean_data['flow'].values,\n",
    "    clean_data['temperature'].values,\n",
    "    clean_data['arrivals'].values,\n",
    "    clean_data['departures'].values,\n",
    "    clean_data['weight'].values,\n",
    "    clean_data['humidity'].values,\n",
    "    circular_timestamps\n",
    "])\n",
    "#Rotate array so that each row is an entry of features from each attribute\n",
    "X = np.rot90(X, k=-1)\n",
    "\n",
    "#Normalize using Min Max Scaler\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will try to find an epsilon value for our DBSCAN model. Since this value represents the minimum distance needed for two points to be classifed in the same cluster, we figured that we could take a high percentile of the distances in order to only classify non-normal behavior in the data. This non-normal behavior is what we are interested, but this method also allows us to cluster healtyh behavior together as one big cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try to find a good epsilon value\n",
    "neighbors = NearestNeighbors(n_neighbors=2, metric='euclidean').fit(X)\n",
    "distances = np.array([])\n",
    "for x in X:\n",
    "    distances = np.append(distances, [neighbors.kneighbors([x])[0][0][1]])\n",
    "epsilon = np.percentile(distances, 99.99)\n",
    "print(f'Try epsilon value {epsilon}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the model using the calculated epsilon and a min_sample size of 96. min_sample determines how many points must be clustered together to qualify as a cluster. We chose 96 since that would require a whole days worth of points needed to qualify as non-normal behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get model\n",
    "model = DBSCAN(eps=epsilon, min_samples=96, n_jobs=None, metric='euclidean').fit(X)\n",
    "#Get labels\n",
    "labels = model.labels_\n",
    "#Print number of clusters\n",
    "clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print(f'There are {clusters} cluster(s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we graph each attribute by time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph each attribute\n",
    "for attribute, column in [['Flow', 6], ['Temperature', 5], ['Arrivals', 4], ['Departures', 3], ['Weight', 2], ['Humidity', 1]]:\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(f'{attribute} vs Time', fontsize=24)\n",
    "    plt.ylabel(f'{attribute} Normalized', fontsize=18)\n",
    "    plt.xlabel('Time', fontsize=18)\n",
    "    for i in range(30):\n",
    "        plt.axvline(x=i/29, c='black')\n",
    "    plt.xlim(right=1.1)\n",
    "    plt.xlim(left=-0.1)\n",
    "    plt.ylim(top=1.1)\n",
    "    plt.ylim(bottom=-0.1)\n",
    "    plt.scatter(timestamps, X[:,column], c=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will graph each individual cluster by its label for each attribute by time. Note that lable -1 is outliers and label 0 is what we classify as normal behavior. We also classified label 4 as having the occurence of a harvest where honey was removed from the hive willingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Graph each attribute per label\n",
    "for attribute, column in [['Flow', 6], ['Temperature', 5], ['Arrivals', 4], ['Departures', 3], ['Weight', 2], ['Humidity', 1]]:\n",
    "    for label in range(-1, clusters):\n",
    "        plt.figure(figsize=(10,10))\n",
    "        indexes = [index for index, lbl in enumerate(labels) if lbl == label]\n",
    "        plt.title(f'{attribute} vs Time for Label: {label}', fontsize=24)\n",
    "        plt.ylabel(f'{attribute} Normalized', fontsize=18)\n",
    "        plt.xlabel('Time', fontsize=18)\n",
    "        for i in range(30):\n",
    "            plt.axvline(x=i/29, c='black')\n",
    "        plt.xlim(right=1.1)\n",
    "        plt.xlim(left=-0.1)\n",
    "        plt.ylim(top=1.1)\n",
    "        plt.ylim(bottom=-0.1)\n",
    "        plt.scatter(timestamps[indexes], X[:,column][indexes], c=labels[indexes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of curiosity, we are going to create a predictive model that will take in all the features except for flow (temperature, arrival, departures, weight, humidity) and attempt to predict flow. Let's start with getting the training data and the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a numpy array to store our features in\n",
    "data = np.array([\n",
    "    clean_data['flow'].values,\n",
    "    clean_data['temperature'].values,\n",
    "    clean_data['arrivals'].values,\n",
    "    clean_data['departures'].values,\n",
    "    clean_data['weight'].values,\n",
    "    clean_data['humidity'].values\n",
    "])\n",
    "\n",
    "#Save true labels\n",
    "true_labels = data[:,-1]\n",
    "\n",
    "#Rotate array so that each row is an entry of features from each attribute\n",
    "data = np.rot90(data, k=-1)\n",
    "\n",
    "#Normalize using Min Max Scaler\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "data = min_max_scaler.fit_transform(data)\n",
    "\n",
    "#Split features and labels\n",
    "labels = data[:,-1]\n",
    "features = data[:,:6]\n",
    "\n",
    "#Split data and lebels into training and testing \n",
    "training_data, testing_data, training_labels, testing_labels = train_test_split(features, labels, test_size=0.1, train_size=0.9, shuffle=True)\n",
    "\n",
    "#Encode labels\n",
    "training_label_encoder = preprocessing.LabelEncoder()\n",
    "training_label_encoder.fit(training_labels)\n",
    "encoded_training_labels = training_label_encoder.transform(training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use a KNN Classifier in order to try and predict flow. We will check its accruacy with different error rates. Note that we can justify using normalized values because if we dont we get divide by 0 errors and get a ton of nan values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build model\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "#Fit data\n",
    "model.fit(training_data, encoded_training_labels)\n",
    "\n",
    "#Get encoded predictions\n",
    "encoded_predictions = model.predict(testing_data)\n",
    "decoded_predictions = training_label_encoder.inverse_transform(encoded_predictions)\n",
    "\n",
    "#Check accuracy of model\n",
    "#Note we will allow an error of _%\n",
    "errors = np.absolute((testing_labels-decoded_predictions)/testing_labels)\n",
    "for error in [err for err in range(1,11)]:\n",
    "    accuracy = len(errors[np.where(errors < error/100)])/len(errors)\n",
    "    print(f'{error}% error has accuracy of {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
